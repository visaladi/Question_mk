name: CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: "3.11"
  IMAGE_NAME: ${{ secrets.DOCKER_USERNAME }}/questionmk

jobs:
  test-ollama:
    name: Test (Ollama)
    runs-on: ubuntu-latest
    services:
      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-${{ env.PYTHON_VERSION }}-

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest
      
      - name: Wait for Ollama to be ready (max ~2min)
        run: |
          for i in {1..24}; do
            if curl -fsS http://localhost:11434/api/tags >/dev/null; then
              echo "Ollama is up"; break
            fi
            echo "Waiting for Ollama ($i/24)..."
            sleep 5
          done
          curl -fsS http://localhost:11434/api/tags

      - name: Pull a small Ollama model for CI
        # choose a tiny model to keep CI fast; change if you prefer
        run: |
          curl -s http://localhost:11434/api/pull -d '{"name":"qwen2.5:7b-instruct"}' 
        shell: bash

      - name: Run tests (Ollama)
        env:
          LLM_PROVIDER: ollama
          OLLAMA_URL: http://localhost:11434
          # use local folder tracking to avoid running mlflow server in CI
          MLFLOW_TRACKING_URI: mlruns
          MLFLOW_EXPERIMENT: ci_quick
        run: |
          python -m pytest tests/

  test-gemini:
    name: Test (Gemini)
    runs-on: ubuntu-latest
    steps:
      - name: Check for Gemini key
        run: |
          if [ -z "${{ secrets.GEMINI_API_KEY }}" ]; then
            echo "No GEMINI_API_KEY set, skipping..."
            exit 0
          fi
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install deps
        run: pip install -r requirements.txt pytest
      - name: Run tests (Gemini)
        env:
          LLM_PROVIDER: gemini
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: python -m pytest tests/

  build-and-push:
    name: Build & Push Image
    runs-on: ubuntu-latest
    needs:
      - test-ollama
      # build waits for gemini tests only if they ran
      - test-gemini
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Docker login (Docker Hub)
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Set tags
        id: meta
        run: |
          SHA_TAG=${GITHUB_SHA::7}
          echo "sha_tag=${SHA_TAG}" >> $GITHUB_OUTPUT
          echo "tags=${{ env.IMAGE_NAME }}:latest,${{ env.IMAGE_NAME }}:${SHA_TAG}" >> $GITHUB_OUTPUT

      - name: Set up Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build & push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
