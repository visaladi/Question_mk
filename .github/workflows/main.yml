name: CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: "3.11"
  IMAGE_NAME: ${{ secrets.DOCKER_USERNAME }}/questionmk

jobs:
  test-ollama:
    name: Test (Ollama)
    runs-on: ubuntu-latest
    services:
      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434
        options: >-
          --health-cmd="curl -s http://localhost:11434/api/tags || exit 1"
          --health-interval=10s --health-timeout=5s --health-retries=30
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-${{ env.PYTHON_VERSION }}-

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest

      - name: Pull a small Ollama model for CI
        # choose a tiny model to keep CI fast; change if you prefer
        run: |
          curl -s http://localhost:11434/api/pull -d '{"name":"llama3.2:1b"}' || \
          curl -s http://localhost:11434/api/pull -d '{"name":"mistral"}'
        shell: bash

      - name: Run tests (Ollama)
        env:
          LLM_PROVIDER: ollama
          OLLAMA_URL: http://localhost:11434
          # use local folder tracking to avoid running mlflow server in CI
          MLFLOW_TRACKING_URI: mlruns
          MLFLOW_EXPERIMENT: ci_quick
        run: |
          pytest -q

  test-gemini:
    name: Test (Gemini)
    runs-on: ubuntu-latest
    # Only run if a key is configured in repo/environment secrets
    if: ${{ secrets.GEMINI_API_KEY != '' }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('requirements.txt') }}

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest

      - name: Run tests (Gemini)
        env:
          LLM_PROVIDER: gemini
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          MLFLOW_TRACKING_URI: mlruns
          MLFLOW_EXPERIMENT: ci_quick
        run: |
          pytest -q

  build-and-push:
    name: Build & Push Image
    runs-on: ubuntu-latest
    needs:
      - test-ollama
      # build waits for gemini tests only if they ran
      - test-gemini
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Docker login (Docker Hub)
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Set tags
        id: meta
        run: |
          SHA_TAG=${GITHUB_SHA::7}
          echo "sha_tag=${SHA_TAG}" >> $GITHUB_OUTPUT
          echo "tags=${{ env.IMAGE_NAME }}:latest,${{ env.IMAGE_NAME }}:${SHA_TAG}" >> $GITHUB_OUTPUT

      - name: Set up Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build & push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
